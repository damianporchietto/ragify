# Ragify Configuration File
# This file contains all configurable parameters for the RAG system

# Server Configuration
server:
  port: 5000
  debug: false  # Set to true only for development
  host: "0.0.0.0"
  # Security settings
  max_content_length: 16777216  # 16MB max request size
  rate_limit:
    enabled: true
    requests_per_minute: 5
    burst_limit: 2

# Model Configuration
models:
  # Default providers and models
  defaults:
    llm_provider: "vertexai"  # openai or vertexai
    llm_model: "gemini-2.5-flash"  # Uses provider default if null
    embedding_provider: "vertexai"  # openai or vertexai
    embedding_model: "text-multilingual-embedding-002"  # Uses provider default if null
    temperature: 0
    openai_api_key: null # OpenAI API Key (can be overridden by OPENAI_API_KEY env var)
    google_service_account_path: "./vertex_service_account.json"  # Path to Google service account JSON file
  
  # Provider-specific default models
  provider_defaults:
    openai:
      llm_model: "gpt-4o-mini"
      embedding_model: "text-embedding-3-large"
    vertexai:
      llm_model: "gemini-2.5-flash"
      embedding_model: "text-multilingual-embedding-002"
      project: "development-466318"  # Google Cloud project ID
      region: "us-east5"  # Google Cloud region

# Document Processing Configuration
document_processing:
  chunk_size: 1500  # Increased to capture more context per chunk
  chunk_overlap: 300  # Increased overlap to ensure continuity
  supported_extensions:
    - "json"
    - "txt"
    - "md"
    - "pdf"

# Retrieval Configuration
retrieval:
  search_type: "mmr"  # "similarity" or "mmr" (Maximal Marginal Relevance for diversity)
  top_k_results: 3  # Increased from 1 to provide more context while maintaining quality
  mmr_diversity_score: 0.3  # Lower diversity score to prioritize relevance over diversity

# Chat History Configuration
chat:
  history_length: 5  # Number of previous interactions to include as context
  max_context_tokens: 2000  # Maximum tokens for chat history context

# Vector Store Configuration
vector_store:
  provider: "qdrant"  # qdrant or faiss
  qdrant_url: "http://localhost:6333"
  qdrant_collection_name: "ragify"
  qdrant_distance_metric: "Cosine"

# Testing Configuration
testing:
  default_timeout: 60
  health_check_timeout: 5
  default_api_url: "http://localhost:5000"
  default_questions:
    - "What is the capital of France?"
    - "How do I make sourdough bread?"
    - "What equipment do I need for a podcast?"
    - "What are the best practices for data security?"
    - "What are the steps to create a website?"

# System Prompt Template
prompts:
  query_rewrite_template: |
    You are a query rewriting assistant. Your task is to rewrite the user's current question to include relevant context from the conversation history, making it a standalone, comprehensive query for document retrieval.

    Previous conversation:
    {chat_history}

    Current user question:
    {current_question}

    Instructions:
    1. If the current question refers to something mentioned earlier (like "it", "that", "this", "the previous answer"), incorporate the specific context
    2. If the question is a follow-up, combine it with relevant information from the conversation
    3. If the question is completely new and unrelated to the conversation, just return the original question
    4. Keep the rewritten query concise but comprehensive
    5. Maintain the user's intent and language

    Rewritten query:

  rag_template: |
    CRITICAL INSTRUCTION: You must ONLY use information from the provided context below. Do not use any external knowledge or make assumptions beyond what is explicitly stated in the context.

    You are a helpful assistant in your late 20s—smart, grounded, and easy to talk to.
    You are a polyglot, please answer in the language of the user.
    Think of yourself as the kind of person someone would message for advice or explanations they can actually understand.
    You're knowledgeable, but never show-offy. Friendly, but not fake.
    The kind of vibe you'd get from someone explaining things over a quiet coffee or casual DM thread—not a lecture, not a sales pitch.

    STRICT RULES:
    1. ONLY use information from the context provided below
    2. If the context doesn't contain relevant information for the question, clearly state "I don't have that information in the available context"
    3. Do NOT guess, infer, or fill in blanks with external knowledge
    4. If context seems irrelevant to the question, acknowledge this and don't force an answer
    5. Ignore any context that doesn't directly relate to the user's question

    Your style is:
    - Casual, clear, and conversational. No robotic phrasing.
    - Natural-sounding responses, like a smart friend talking—not a corporate bot or tech manual.
    - No bullet points or numbered lists. Just smooth, direct explanations.
    - Short intros or small talk are fine if it helps the response feel human, but don't ramble.
    - Be honest. If something's missing, say so.

    Context:
    {context}

    User Question:
    {query}

    Your task:
    Respond naturally, like a real person would in a chat. Use ONLY the information in the context above.
    If the context doesn't contain relevant information for this question, say so directly. Keep it warm, smart, and human.

    Answer:

# API Documentation Template
api_docs:
  title: "Ragify - RAG Framework API"
  description: "API for knowledge-based question answering using Retrieval Augmented Generation."
  # The full HTML template will be loaded from a separate file for better maintainability
  
# Directory Paths
paths:
  docs_dir: "docs"
  storage_dir: "storage"
  results_dir_prefix: "results"

# Environment Variables Mapping
# These define which environment variables override which config values
env_overrides:
  OPENAI_API_KEY: null  # Always from env
  PORT: "server.port"
  FLASK_DEBUG: "server.debug"
  LLM_PROVIDER: "models.defaults.llm_provider"
  LLM_MODEL: "models.defaults.llm_model"
  EMBEDDING_PROVIDER: "models.defaults.embedding_provider"
  EMBEDDING_MODEL: "models.defaults.embedding_model"
  OPENAI_LLM_MODEL: "models.provider_defaults.openai.llm_model"
  OPENAI_EMBEDDING_MODEL: "models.provider_defaults.openai.embedding_model"
  GOOGLE_SERVICE_ACCOUNT_PATH: "models.defaults.google_service_account_path"
  VERTEX_PROJECT: "models.provider_defaults.vertexai.project"
  VERTEX_REGION: "models.provider_defaults.vertexai.region"
  VERTEX_LLM_MODEL: "models.provider_defaults.vertexai.llm_model"
  VERTEX_EMBEDDING_MODEL: "models.provider_defaults.vertexai.embedding_model"