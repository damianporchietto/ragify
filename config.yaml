# Ragify Configuration File
# This file contains all configurable parameters for the RAG system

# Server Configuration
server:
  port: 5000
  debug: true
  host: "0.0.0.0"

# Model Configuration
models:
  # Default providers and models
  defaults:
    llm_provider: "openai"
    llm_model: null  # Uses provider default if null
    embedding_provider: "openai" 
    embedding_model: null  # Uses provider default if null
    temperature: 0
  
  # Provider-specific default models
  provider_defaults:
    openai:
      llm_model: "gpt-4o-mini"
      embedding_model: "text-embedding-3-large"
    ollama:
      llm_model: "mistral"
      embedding_model: "nomic-embed-text"
    huggingface:
      llm_model: "google/flan-t5-xxl"
      embedding_model: "BAAI/bge-large-en-v1.5"
      pipeline_max_length: 512

# Document Processing Configuration
document_processing:
  chunk_size: 1000
  chunk_overlap: 200
  supported_extensions:
    - "json"
    - "txt" 
    - "md"
    - "pdf"

# Retrieval Configuration
retrieval:
  search_type: "similarity"
  top_k_results: 4

# Testing Configuration
testing:
  default_timeout: 60
  health_check_timeout: 5
  default_api_url: "http://localhost:5000"
  default_questions:
    - "What is the capital of France?"
    - "How do I make sourdough bread?"
    - "What equipment do I need for a podcast?"
    - "What are the best practices for data security?"
    - "What are the steps to create a website?"

# System Prompt Template
prompts:
  rag_template: |
    You’re a smart, relaxed assistant in your late 20s. You sound human—like someone who knows their stuff and explains things like you would to a friend over coffee or DMs.

    You only answer questions using the exact context provided—nothing more. If something isn’t in that context, be honest and say you don’t have that info. Never guess, never make things up. You can suggest that the person rephrase the question or check another source, but that’s it.

    Keep the tone casual but clear. You’re not a robot, so don’t talk like one. No lists or rigid formats—just smooth, natural explanations. Be direct, helpful, and grounded in what you know.

    Context:
    {context}

    User Question:
    {query}

    Your job:
    Have a real-sounding conversation. Only say what the context gives you. If it’s not there, say so. Keep it human.

    Answer:

# API Documentation Template
api_docs:
  title: "Ragify - RAG Framework API"
  description: "API for knowledge-based question answering using Retrieval Augmented Generation."
  # The full HTML template will be loaded from a separate file for better maintainability
  
# Directory Paths
paths:
  docs_dir: "docs"
  storage_dir: "storage"
  results_dir_prefix: "results"

# Environment Variables Mapping
# These define which environment variables override which config values
env_overrides:
  OPENAI_API_KEY: null  # Always from env
  PORT: "server.port"
  FLASK_DEBUG: "server.debug"
  LLM_PROVIDER: "models.defaults.llm_provider"
  LLM_MODEL: "models.defaults.llm_model"
  EMBEDDING_PROVIDER: "models.defaults.embedding_provider"
  EMBEDDING_MODEL: "models.defaults.embedding_model"
  OPENAI_LLM_MODEL: "models.provider_defaults.openai.llm_model"
  OPENAI_EMBEDDING_MODEL: "models.provider_defaults.openai.embedding_model"
  OLLAMA_LLM_MODEL: "models.provider_defaults.ollama.llm_model"
  OLLAMA_EMBEDDING_MODEL: "models.provider_defaults.ollama.embedding_model"
  HF_LLM_MODEL: "models.provider_defaults.huggingface.llm_model"
  HF_EMBEDDING_MODEL: "models.provider_defaults.huggingface.embedding_model" 